{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["In the name of God<br>\n", "Sajjad Haghighat"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class MLP(object):\n", "     def __init__(self, data, layers_size , act_func , etha = 0.1  , rand_scale = 0.1):\n", "        self.dataset = data\n", "        self.layers_size = layers_size\n", "        self.etha = etha\n", "        self.rand_scale = rand_scale\n", "        self.neti = [None] * (len(layers_size)-1)\n", "        self.oi = [None] * (len(layers_size) - 1)\n", "        self.delta = [None] * (len(layers_size) - 1)\n", "        self.act_func = self.sigmoid if act_func==\"sigmoid\" else self.relu\n", "        self.der_func = self.sigmoid_derivative if act_func == \"sigmoid\" else self.relu_derivative\n", "        self.mean_squared_error = []\n", "        self.create_weights()\n", "     def create_weights(self):\n", "         nl = self.layers_size\n", "         self.wi = [None] * (len(nl) - 1)\n", "         for i in range(len(self.wi)):\n", "             self.wi[i] = np.random.normal(0, self.rand_scale, (nl[i+1], nl[i]+1))\n", "         self.wi_new = self.wi\n", "     def train(self , epoches=1):\n", "         for epoch in range(epoches):\n", "             for row in self.dataset:\n", "                Y = row[0]\n", "                X = np.append(row[1:] / 255, 1)\n", "                network , expected = self.feedforward(X, Y)\n", "                self.backpropagation(X, network, expected)\n", "         print(self.wi)\n", "     def feedforward(self, X, Y):\n", "         # Convert To 2D Array for compute Transpose\n", "         X = np.array(X , ndmin=2)\n", "         nl = len(self.layers_size)\n", "         for i in range(nl-1):\n", "            net = np.dot(self.wi[i], X.T)\n", "            self.neti[i] = net[:, 0]\n", "            tmp = self.act_func(net)\n", "            if i == nl-2:\n", "                self.oi[i] = tmp[:, 0]\n", "            else:\n", "                self.oi[i] = np.append(tmp, 1)#bias\n", "                X = np.array(self.oi[i], ndmin=2)\n", "         Y = self.squared_error(self.oi[-1],Y)\n", "         return self.oi[-1] , Y"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["     def backpropagation(self ,X, network , expected):\n", "         for i in range(len(self.layers_size)-2,-1,-1):\n", "             if i == len(self.layers_size)-2:\n", "                 # last layer\n", "                 delta = -2 * self.der_func(self.neti[i]) * (expected - network)\n", "                 self.delta[i] = delta\n", "                 wi = self.wi[i]\n", "                 for j in range(len(delta)):\n", "                     RondE = self.oi[i - 1] * delta[j]\n", "                     wi[j] = wi[j] - self.etha * RondE\n", "                 self.wi_new[i] = wi\n", "             else:\n", "                 #Hidden layers\n", "                 wi = self.wi[i+1]\n", "                 di = self.delta[i+1]\n", "                 delta =  self.der_func(self.neti[i]) * [sum( wi[:,j] * di) for j in range(wi.shape[1] - 1)]\n", "                 self.delta[i] = delta\n", "                 wi = self.wi[i]\n", "                 for j in range(len(delta)):\n", "                     RondE = self.oi[i - 1] * delta[j] if i > 0 else X * delta[j]\n", "                     wi[j] = wi[j] - self.etha * RondE\n", "                 self.wi_new[i] = wi\n", "         self.wi = self.wi_new\n", "     def squared_error(self, o, d):\n", "         tmp = np.zeros(10)\n", "         tmp[d] = 1\n", "         d = tmp\n", "         self.mean_squared_error = np.append(self.mean_squared_error,((d - o) ** 2).mean())\n", "         return d\n", "     def mse(self):\n", "         print(\"MSE : \",self.mean_squared_error.mean())\n", "     def sigmoid(self, z):\n", "         result = 1.0 / (1.0 + np.exp(-z))\n", "         return result\n", "     def relu(self, z):\n", "         if np.isscalar(z):\n", "             result = np.max((z, 0))\n", "         else:\n", "             zero_aux = np.zeros(z.shape)\n", "             meta_z = np.stack((z, zero_aux), axis=-1)\n", "             result = np.max(meta_z, axis=-1)\n", "         return result\n", "     def sigmoid_derivative(self, z):\n", "         result = self.sigmoid(z) * (1 - self.sigmoid(z))\n", "         return result\n", "     def relu_derivative(self, z):\n", "         result = 1 * (z > 0)\n", "         return result\n", "     def softmax(x):\n", "         e_x = np.exp(x - np.max(x))\n", "         return e_x / e_x.sum(axis=0)\n", "     def cross_entropy(predictions, targets, epsilon=1e-12):\n", "         predictions = np.clip(predictions, epsilon, 1. - epsilon)\n", "         N = predictions.shape[0]\n", "         ce = -np.sum(targets * np.log(predictions + 1e-9))\n", "         return ce\n", "     #def print_norm(self):"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    X = pd.read_csv(\"mnist_train.csv\")\n", "    print(X.head(2))\n", "    X = X.to_numpy()\n", "    mlp = MLP(\n", "        X,\n", "        [784, 32, 16, 10],\n", "        \"relu\"\n", "    )\n", "    print(\"Training..........................\")\n", "    mlp.train()\n", "    mlp.mse()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}